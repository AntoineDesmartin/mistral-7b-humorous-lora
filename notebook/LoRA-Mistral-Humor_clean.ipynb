{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5c714651-8499-4daa-c2cf-6cea39dac422"
   },
   "outputs": [],
   "source": [
    "!pip -q install -U \"transformers>=4.41.0\" \"accelerate>=0.30.0\" \"bitsandbytes>=0.43.0\" \"peft>=0.11.1\" datasets evaluate sentencepiece wandb\n",
    "\n",
    "import torch, subprocess, sys, platform\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "!nvidia-smi || echo \"Pas de GPU NVIDIA (ok en Colab GPU).\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "260ecce8c0f745b4b1d8520c993090b0",
      "1c1b44408234400886b7d33c82775ccb",
      "735f52a0b54c4715a7bf143adbdf1e54",
      "d1dbda23912d4a8e90e3e7bc2253c377",
      "770f9f9df3a147139af2f434ae5f3dad",
      "1d98ed4a8dd0429181c03d69b224b0f0",
      "730db7efc2b24972b933cd81f68b49ff",
      "12f0d8c4337c439db36ad2952b2551b5",
      "ca6b4fc1dd0a4e3581e0c79ccfaf6c31",
      "e1a3e2c75eed42ffa3ef6cca2a2abe52",
      "9dd8cb4aac2f4952987eeb3da8a5d23b",
      "c2acb4d98758455d83167286da5141c0",
      "638277ef7ace4801a13a1667646456c4",
      "376ae1a16b034e35878762d736498ee0",
      "fa205c0e9bb24bde83ad87fba5d219df",
      "1ac3fc2907f3458d9934fbb587ba5b27",
      "a5fc6b5358124b18acdbf177a5610e03",
      "98ec243c6a3249e7aea0f6b22fde31ee",
      "84602ea74e3a4e95bf02efb0c88a32fc",
      "ec00e28f01b440fe8fdabfc9809a461d"
     ]
    },
    "outputId": "d6000de6-8201-4d2c-fe14-98888e63bfb0"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "outputId": "ced29703-7367-4225-cb15-770b2b3738ab"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6e5bdf94-62fe-4cd9-b879-d13497cc0d54"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"  \n",
    "PROJECT_DIR = \"lora-mistral-humor\"\n",
    "DATA_PATH   = \"persona_qa (3).jsonl\"  \n",
    "HF_REPO_ID  = \"ton-username/mistral-7b-humorous-lora\"\n",
    "\n",
    "SEED = 42\n",
    "MAX_LEN = 512\n",
    "NUM_EPOCHS = 3\n",
    "LR = 2e-4\n",
    "TRAIN_BS = 4\n",
    "EVAL_BS = 4\n",
    "GRAD_ACCUM = 1\n",
    "WARMUP_RATIO = 0.03\n",
    "LOG_STEPS = 50\n",
    "\n",
    "import random, numpy as np\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c1d88d96-3314-449f-8dab-ed2e4c277d26"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"json\", data_files=DATA_PATH)  \n",
    "ds = data[\"train\"].train_test_split(test_size=0.1, seed=SEED)\n",
    "train_raw, eval_raw = ds[\"train\"], ds[\"test\"]\n",
    "len(train_raw), len(eval_raw), train_raw[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180,
     "referenced_widgets": [
      "b8559bbf58384854864b6bffa8491689",
      "a323b432911b4f5f8f91465209b29d5d",
      "7baa003204f4461ab42d45b5f7713b4c",
      "8805627337a84d738a4214c06420c81e",
      "dbced659bb0c4a1383bc088d9395d7fe",
      "9b044c321321439e941bc96c17085587",
      "edd599723667493da4b1c6701aac4e8e",
      "1f33776149014efdb1eeabf262bdf7af",
      "e63f863e82d846d3b6d82759dd8e00ab",
      "7a8871bbbbac4c76ac554e9bb23e67cd",
      "ad11127bdc0047d9a7d591b87e7ea728",
      "ad9c3b92e09f4bc48e92f25f546aeb6b",
      "6acd7fea066f442d87d8be878af2f28b",
      "36a521c2b9c64e08b5100e3d737932e9",
      "0d83eeb8b56841f2aa0e00d7913a629a",
      "986224e48a8245948677e50146b94757",
      "de898b8a7090493a8932ebc931ce34fd",
      "4588c212903e4ee89a6c90170c78ad8b",
      "2fba926719f945e98bff311851666b20",
      "2e343799a95b4a4a8a24e77e9dc915ca",
      "5eb54e7fc58c42f9b1bf08b158b28f58",
      "7525bea82a06415c9cadbaaa02412c4d",
      "3394c28a230f4cd2a7e92981102f6641",
      "f07c97f5bcb141b4ab002b5c30db3c9f",
      "f69e0910dd2f41b2b152e7e67453ed47",
      "e343357c8fc84402b33da8d6fed1a2b0",
      "1c1e6ceabaf84275ba36a5176c18bdad",
      "570c67dfbe034363a28363bb28e17b26",
      "a32652c74c7b46d88372ed6fb6e7da68",
      "7c5c8ec2fe6d4f519332be246e7386f5",
      "96a756fa8dd44412bd23d29279495255",
      "28c5c332e7e14abd86a1093d4409e35d",
      "42f6cd449ea5424fb3dbb50a0b53e0a1",
      "edc20682959f4979812b0964fc4326fa",
      "19e9e604036648f9ab1e5cf51bbb0815",
      "b9eba5ef5d89469b9a712d064b8e7799",
      "84be9a9aab8547dbb97b257544416044",
      "aaa26223eb824828b56ee2af3e0e201f",
      "ee23adbb0a2243a386f08449d6c8a384",
      "a113b4278cff498cbbb4894a62490e5d",
      "4ee6d80d9d3f420881c5b8b18a011c71",
      "b11f6bda5ea3411bab06013f31c43110",
      "df479e2fce28422585889482bb2d83b4",
      "0ce6255b0fe048269b8785bbd2872416"
     ]
    },
    "outputId": "b9307992-00b7-4ab6-b553-7102ac5cb983"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(\"BOS/EOS:\", tokenizer.bos_token, tokenizer.eos_token)\n",
    "print(\"Has chat_template?\", tokenizer.chat_template is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512  \n",
    "\n",
    "def build_prompt(instruction: str):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": instruction.strip()}],\n",
    "        tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "def encode_example(example):\n",
    "    prompt = build_prompt(example[\"instruction\"])\n",
    "    answer = (example.get(\"output\") or \"\").strip()\n",
    "\n",
    "    full = tokenizer(\n",
    "        prompt + answer,\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",      \n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    input_ids = full[\"input_ids\"]\n",
    "    attention_mask = full[\"attention_mask\"]\n",
    "    labels = input_ids.copy()\n",
    "\n",
    "    for i, m in enumerate(attention_mask):\n",
    "        if m == 0:\n",
    "            labels[i] = -100\n",
    "\n",
    "    prompt_ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "    pl = min(len(prompt_ids), len(labels))  \n",
    "    for i in range(pl):\n",
    "        labels[i] = -100\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "25de7856bca54fd7a976d7cc65afda51",
      "b366242c9b3c47a1aa182aff341ec72a",
      "9f0c9840f26b4ed18a53b12c3d55995e",
      "44e302a18b304bbca30d2a86a414d882",
      "c597e29521b5432c9204c6301fcbcd03",
      "8fe265ef41e8490b89eec5c7e2f055e0",
      "2097dcfb5e634792a4c6c4457309deda",
      "bcbe0ad4049a40c49f53b46a0ee81fbc",
      "99f5d001498d4a1c9d6bcfc170d1a68f",
      "f9016f7c541d45218a1e6f6b315c40a6",
      "79a031b580e445afba225041a4c2aa0c",
      "679a86f60c1f4db5bbf910f9a0a5c4d7",
      "1d4d5f0a6b9e4172b84db077c65586f7",
      "f1796aa4afdd49c0bb7d6ac1466b794f",
      "ff15f72b495a45fab91894acb8568dad",
      "19228b17d4914ed28393329b633d58ad",
      "b66af3e6722847e48648a030ac6af0e9",
      "e8f046a0179f46b492ca7e37f8a2860b",
      "f89e345381a94e2a91d72b0864e48c8c",
      "59a60e2a954c4c91856e358042ac5d73",
      "95ced8825144493b8cc90d41547fa777",
      "81b96aa6d7bf4359a4745de015e86013"
     ]
    },
    "outputId": "a07381b7-3062-4a59-ba0d-77982cd7db3c"
   },
   "outputs": [],
   "source": [
    "cols_to_remove = train_raw.column_names\n",
    "train_ds = train_raw.map(encode_example, remove_columns=cols_to_remove, desc=\"Tokenizing train (fixed)\")\n",
    "eval_ds  = eval_raw.map(encode_example,  remove_columns=cols_to_remove, desc=\"Tokenizing eval (fixed)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "669c25a7-8258-4f85-b341-2fcd55f0aa70"
   },
   "outputs": [],
   "source": [
    "def check_lengths(ds, name):\n",
    "    bad = []\n",
    "    for i, ex in enumerate(ds):\n",
    "        if len(ex[\"input_ids\"]) != MAX_LEN or len(ex[\"labels\"]) != MAX_LEN or len(ex[\"attention_mask\"]) != MAX_LEN:\n",
    "            bad.append(i)\n",
    "    print(f\"{name}: OK={len(ds)-len(bad)} / {len(ds)} ; BAD={len(bad)}\")\n",
    "    return bad\n",
    "\n",
    "bad_train = check_lengths(train_ds, \"train\")\n",
    "bad_eval  = check_lengths(eval_ds,  \"eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator, TrainingArguments, Trainer\n",
    "\n",
    "data_collator = default_data_collator  \n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=PROJECT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LR,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    logging_steps=LOG_STEPS,\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_strategy=\"epoch\",          \n",
    "    save_strategy=\"no\",\n",
    "    fp16=True, bf16=False,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    "    remove_unused_columns=False,    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273,
     "referenced_widgets": [
      "935bd22ceb5f4a9db8366184c199a4af",
      "a2a4f5c808314efb9cc39bb1092d1e26",
      "7e4e15d620ae48d0ad121b07bedb0ff4",
      "6482526f066c40708026f0fd30dfdf3b",
      "2a3da5a20ad14475a2bb51152676f2e3",
      "6ea9b25b6d384f28a8894ab21e98163c",
      "b9801dd7bb4849a69043f0c9e7c959fd",
      "1264f163cf0f4336a11bf4edbf829ebf",
      "71f445e10ae74a42894c6af890fa2b3f",
      "32822d18ab2d4b9e95a940b1bf026fb9",
      "4e88d3bb239a46a7a7090fddf9a59576",
      "0e77db7530024f78add0686ce8dbb876",
      "0cf7454f2bab42338daf968d0efd0a59",
      "95ab9d49cd81450c9fd09ece4251ecba",
      "d1999bda4fb047c5b92448f7c1dbebfa",
      "49d6b6d3b6e3414d8c52da0de319f67a",
      "34f6f540ee0146c887359c24d053f1b6",
      "a9559707d14d4a4093349c3ae51aeebe",
      "f5077ce596f74b1399f70d135edf3353",
      "e07699ebf8d84e0f87ede5be7f4845a0",
      "088af28b091542a7b7859b288d0cfe83",
      "0b7224b9c36e44d7b4c2fb8a323102f3",
      "48c8d50d09e9448aa8e72ee3b23c442e",
      "7198e628a8d74643bd84e15baadeefbc",
      "168e3177cdab4839866c12b578bc78c2",
      "d19deab79c94454f97315064dac5e46b",
      "0b302894dac44d2b8cc8339fe089d85c",
      "83f7c519e46a487dbc0807fe75ccaab9",
      "80162fd139614e77b08e7e7488822d9c",
      "a433114807e84350a060cbfc8cb35a4e",
      "cf34b8d9c5d64fefa513da6cbb05bd80",
      "c948e12948aa4b5cb372d42a4ef1ad25",
      "f395d3812b824e09b79933334e774d4a",
      "720ded5c53074ce59951c2d24fc8ac41",
      "044874d7de7d4c7db0964b6518e30c9c",
      "eb239a9dcbcf4cac9702fb336e0d0272",
      "84d1006a6129461bbe07ab2d6f0ff689",
      "a38b485850d448eab72fb56dd818c2c0",
      "149e8801c0d5445cbaa29b6e7cf1b99e",
      "74bbf94d7fea418089cec381fc93f54a",
      "396309744ad44deeb9e2830c7a32bf85",
      "e6114c55a9fb4868805cbbb2d930ac56",
      "28a6628cfb28436fb1116f9a7ac640a9",
      "9593dd0df558403c81386d8c50fb8f30",
      "1ba354fbfdec4494912945376fd9db05",
      "be89e464bd114d86ad26648a6b3068ae",
      "356f61a9d14d4520a0a9ea9bb5f214e7",
      "f199fb6b5646454d999630cb10b1f1c1",
      "fdc5561aebd0454d978c0f603d573d96",
      "c227f5ffd2a146e8a23cd4e2a7aa898a",
      "28fed2f1cab04cc7a88574a17b464d08",
      "c65e814c0e84457da63e4dbb962ab796",
      "7afeae0f999947788f0bd66520362a0d",
      "59f163317cb14c00b74e0313157bd6ab",
      "d9ee9675c58a44beab4692cba1e4e0a2",
      "555ae6cf60a74f96b7fb2d3fbaaaad6d",
      "de5eb67c93ba40c0bb1fcca97500e5a8",
      "01cb6460ec5442329ea354bd2f7c2139",
      "4674c8756cf34a0899ca16dfe371ca91",
      "05520027b2974743ad6b7820571ddf85",
      "b197d72470b44158af24faf50fe3a90e",
      "c91fca0f52c24e5bb50cff448c3ba43a",
      "e56c2bdf9acf4b0b8dc076d72f6834b6",
      "c3844c8dfdcc488da78b5702e7563a79",
      "76fcaadf92a1451a9db4276efb7c56fb",
      "e89fd3854e514d88bcabd49e373b10af",
      "255b1878809340378540f49b194ba768",
      "69064eabdd2a44d4b6b706718e3864c2",
      "606a26b4c0b043aa8765cd6ed8dd9ccd",
      "c99ce5e835ec45b89e088b447628f74e",
      "f20444d054794193a42ec906f1f9bedb",
      "609e0083ce834207a48385d375e8799f",
      "6e6dc62ec9844de6a07c570fcee694bd",
      "85a73f72a59f403aafd46f195ab5b30a",
      "e3168ce30a154ba8a70483818023730c",
      "6b190e153cfe4d61a5d5511736bad824",
      "5896812071ed4110b43e5a73210e2863",
      "beae28971abb488e86c03c26882a9160",
      "7ac4b0fe6c374dfe993018bd90344b20",
      "a1aa3f6156f345109b0e273558bd9921",
      "584e8756578249ffaab7e22272d48dfe",
      "c9967307a6c44655954f62c4686d0a98",
      "3afd99d2c10440f9b8e6f438b945409e",
      "06bfd912e3794a219a2ebce80a3a1a2b",
      "8ca3076ebd0d40019f47d19d9ef4ce7e",
      "ec2af5900b2a432ea46ab9cb00ba7688",
      "6c47b092d5fd4ea29ca19cfc40d816f4",
      "185897e9ddd442719c79c9b4d7c22bc7"
     ]
    },
    "outputId": "cfdcb3e3-ba17-40dc-877c-8a0135f8336a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16, \n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_cfg,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.config.use_cache = False  \n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()  \n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "model = prepare_model_for_kbit_training(model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6da1cbd3-bec9-4b15-927e-6a336852885a"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "def print_trainables(m):\n",
    "    trainable, total = 0, 0\n",
    "    for p in m.parameters():\n",
    "        n = p.numel()\n",
    "        total += n\n",
    "        if p.requires_grad:\n",
    "            trainable += n\n",
    "    print(f\"Trainable params: {trainable:,} / {total:,} ({trainable/total*100:.2f}%)\")\n",
    "print_trainables(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "outputId": "0690fd06-431c-4964-ad57-e83e0792ac3c"
   },
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "fd7d792189de40cc86bf382d91ae0bdd",
      "33c8e1b06cfe44ac8b3258f97f9c5e33",
      "f2b8b2aa3b834054aa1664b09f536de2",
      "39909009b4df4e98bfc13c9df8724334",
      "39f3e02a76564d78935d434a2027698f",
      "32a20020da3c4baea0aa84f5bc9017bd",
      "c587737afb1146789c1e11888038e73b",
      "1049e26c06a44a3a9de639730af87fe1",
      "53d289271c034732807d6cf6ce49cf3d",
      "b01d79ce1e5840dcb8b2cf2460aa0cde",
      "a4a053d50e6b456194f9daeec2b7ca4f"
     ]
    },
    "outputId": "fb6b0076-6ff7-4fe3-f82a-987a8e93b012"
   },
   "outputs": [],
   "source": [
    "import math, re\n",
    "eval_out = trainer.evaluate()\n",
    "eval_loss = eval_out[\"eval_loss\"]\n",
    "ppl = math.exp(eval_loss) if eval_loss < 50 else float(\"inf\")\n",
    "print(f\"Eval loss: {eval_loss:.4f} — Perplexity: {ppl:.2f}\")\n",
    "\n",
    "def normalize_text(s):\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"[^\\w\\s]\", \"\", s)\n",
    "    return s\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "model.eval(); model.config.use_cache = True\n",
    "\n",
    "preds, refs = [], []\n",
    "for ex in tqdm(eval_raw):\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\":\"user\",\"content\": ex[\"instruction\"]}],\n",
    "        tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=128, do_sample=False, temperature=0.0)\n",
    "    gen = tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "    preds.append(gen)\n",
    "    refs.append(ex[\"output\"].strip())\n",
    "\n",
    "em = np.mean([normalize_text(p)==normalize_text(r) for p,r in zip(preds,refs)])\n",
    "print(f\"Exact Match (normalisé) sur test: {em*100:.2f}%\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"\\nQ:\", eval_raw[i][\"instruction\"])\n",
    "    print(\"REF:\", refs[i])\n",
    "    print(\"GEN:\", preds[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "183539ff-97a4-44b5-c180-aae31d4f0dd5"
   },
   "outputs": [],
   "source": [
    "SAVE_DIR = \"persona_humor_lora\"\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "!ls -lh {SAVE_DIR}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258,
     "referenced_widgets": [
      "c6495b77bc7346e89e50d44ee0653a25",
      "5fd28e0530934a80b7da070a32c5197f",
      "e36bbcf8d4164329b3cd7b9e42e0d5e8",
      "6386c426438c4594a0b33a84060a5d51",
      "27eb21e0d5ce4e258815e046c7ca6785",
      "96b632f637234ae79c8855ef3a110041",
      "021e69e7934c4ad9a5ab5604ab8ce912",
      "c2bdea6724b942f89f948d3cc3176236",
      "b9c5ac8faf77444ba1146590f0eb42d2",
      "e4522ba412f344b9aa32e39e453b0266",
      "7abf6a3b27684732a79219c31abf7048",
      "b5d2543cf642401cbfbacf168e650806",
      "98c9e91ea30146199a339c8521be87c8",
      "5417b3b67d844428b659eeced340e1a8",
      "bfe2e45cd98e4d2fa71487098fe94a0f",
      "1118247786ac44b08759df3ef5bfb781",
      "11e17a11d7cd4c18894565a0b8d0e714",
      "2ff6489b715c450280cd8580b3bd5549",
      "72c6c9f1633a4fa89e92af5ea8cfe93e",
      "0941d677a5ba44dd83f3c88de8829aa2",
      "8dc6ad85ad794dbfb9ee6d650d8df461",
      "907e9e3925b44e1cbf330570d904ba14",
      "7b422e695c0d4bdaa045080d44998c08",
      "3e9496801e0e49a08c6b420668146866",
      "2eb6719c4884405280f9ccc0cce555bd",
      "a560bf6c009c42f9a5d539cc2547e941",
      "88fb231fe6a8495ea8f6c16904172940",
      "46287d3ad5b84a2fb7b23da215919df7",
      "974555cb7d264dcbace29684d18d1d61",
      "5705bc698660467e9291975d1294f7c3",
      "4f8d2725ebcb4f40adb14a0a69e63350",
      "a400628f18514c04b5d2b233f1219e1e",
      "bbe0a6bfae4a40d5908033733f66100a",
      "582cf9b488034d14833ce56fd6b8e8b2",
      "e06ac319a95749858165daada516e9c5",
      "5ccde2548c1e4fbdacdb306fd559d5cb",
      "5b7090a2e9ac41f2b7aea82581074110",
      "f5d33f10751443e385536e144c2a1c98",
      "0bc916c49f08462496597a9d311e85d2",
      "0e823c012e444d4b93fe1548314270fe",
      "9238278d74424674b610a5dce0b56c53",
      "6b1017136eff47d592a9388f0ef98ab1",
      "164cae31d368490e95519f5a78e607e7",
      "0a627f5345cb4e21a42a24b0467b295e",
      "55d49848be334e1d817a6b5ea1d0e9aa",
      "22a45a1b1a2b458aae5f26ef67079546",
      "ed18eca1aed4450b9d690974e39a423f",
      "1841326bc79b41e39d1fd425807fce05",
      "1e92cbb612c246e28c1c9e8402bd2358",
      "13ba954afab642cbb961f1a68c95fc43",
      "5d4abe95ef4d4b4f9053245ed9179bbe",
      "650aa46c58ee42f780087185f8106362",
      "75d48fc4ae6246228009da885f1d6eb6",
      "f34cbae0456541e5b60f96504cfdf9d2",
      "34f23ab52fdd42a6a79378e7dc182496",
      "717dc2b5947e4970a6fc56c722199c17",
      "fd98914228144c5080db9b91bc6fe748",
      "e0f554e6f65c437db462074340da7965",
      "8dd3592220c448928476ac2f507ebba5",
      "2ef44e4d0a8a4749aeefd25b01dba6bc",
      "156a9fa1fa5c4235bc552cd14ca849ef",
      "a30b122d12eb4a9b8d9e1c33d50f45ba",
      "242ffd2c23434dc68b4e8a4db8eb76d3",
      "e53ad7f4d8df4ca188aa97a7feb28b07",
      "574ba667472b4ae19c86d8f88a398d9c",
      "4ac17fb09b184e64b419030d6d3c3535",
      "cedf017eed014bc2a1691d0d1ac5afbb",
      "f8583c80a54c4e32b497a68f749d7b08",
      "2a74775d0eab4d3389feae0fe338aa5c",
      "cfc23df30f3a41438b5e481d6878f0cf",
      "2ed2df95e7fe47d3b37ddac8c505efc0",
      "ec39ae6c96ff4c08a52c2eb3aad76ebd",
      "1dfac2da2ddb4cb1a7919ded6678ead6",
      "30427e2868e24d2a8ed2b2d29a25255e",
      "d7a72730d2ca434e894635a3c170ed86",
      "19b7fafdaeb94433bd430a9b68754127",
      "860454ec7ded43e7863f18465f71eb77"
     ]
    },
    "outputId": "975bcf75-99a2-4785-f268-a19fdf3d2eb8"
   },
   "outputs": [],
   "source": [
    "HF_REPO_ID = \"tonioexe/mistral-7b-humorous-lora\" \n",
    "\n",
    "model.push_to_hub(HF_REPO_ID)\n",
    "tokenizer.push_to_hub(HF_REPO_ID)\n",
    "print(\"Poussé sur:\", HF_REPO_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168,
     "referenced_widgets": [
      "0468986e9bd14e75880e1f5747109111",
      "7b44531e9fb848f882527439e859474d",
      "f373cd3a33164f0d956f0f39b60c5027",
      "996ad8a65b4f42d59fd1cdf15fba5c5b",
      "4bf870fbc23840ce8ef8f52380a6f6c1",
      "7e399bd8e02541afa58d3b8959fa4220",
      "7063e7ff3c524e67b5583a039a5e1027",
      "c85c2b7b70334bbaa8bf6ac8dd005b21",
      "50c731d48acb477292aa6cca833944e7",
      "58f12087accf479cb04891c985548664",
      "f7c30f413a2341aebc4bab9feade5f01",
      "97a042530be84e4ea509da0e3fb9ee76",
      "503323cdea974dc4b018fa98bc2c4fc2",
      "2516923a5e5a4cd1a774bdc82f82b519",
      "2a420a3558514460ae2b8e844ee2f5a2",
      "284ff2d48c344258b372f2554de20519",
      "fa00a6522318417d9cc2e530655ab6f2",
      "a1ef473ffd454791af8946d8e337bddc",
      "cbb03bded56446f49d23988d3b3f49dd",
      "1c7a339ddccd46fb86eafd231aca5369",
      "9787d7ec539f41838047e4f7cc256d90",
      "859fb0061b834000aa341bcb3f34f32c",
      "1091f8bd19da4f3da982c619bca39d89",
      "a16e1d6c9a9b4b01b9199093efcf2f09",
      "e96c3107bbeb4b38855d4ebc40a5dbc6",
      "7e3e9a7f95224f62b08831a6bf969a52",
      "2127b23b54ae4efb9662df7b266b42bc",
      "5b8dee9c9a114e559633fba28ad68dbc",
      "d5a5c769565d404b8fc90062280b71fc",
      "fb0ede99d56c48c6a6816490c9ff1bb0",
      "5771c12c9a214cec911021822e515c72",
      "4663622bf0a94dc58f263f60876401d6",
      "8a01d885ccbc4ae68d4d7491d45c818b"
     ]
    },
    "outputId": "599bf672-d600-4d9c-d9e4-a2cc47e79155"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\",\n",
    "    quantization_config=bnb_cfg, trust_remote_code=True\n",
    ")\n",
    "tok  = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "\n",
    "model_peft = PeftModel.from_pretrained(base, HF_REPO_ID)\n",
    "model_peft.eval(); model_peft.config.use_cache = True\n",
    "\n",
    "def chat(question):\n",
    "    prompt = tok.apply_chat_template([{\"role\":\"user\",\"content\": question}], tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tok([prompt], return_tensors=\"pt\").to(model_peft.device)\n",
    "    out = model_peft.generate(**inputs, max_new_tokens=128, do_sample=True, top_p=0.9, temperature=0.7)\n",
    "    return tok.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "print(chat(\"Quick snack ideas for gut health?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a66e66b4-0a33-4588-d4b3-aa2e8ab7d649"
   },
   "outputs": [],
   "source": [
    "merged = model_peft.merge_and_unload()  \n",
    "CPU_DIR = \"mistral7b_humor_merged_fp16\"\n",
    "merged.save_pretrained(CPU_DIR)\n",
    "tok.save_pretrained(CPU_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "984ade9a751e4aa38ae8d391e161559b",
      "61073a1388bc49cfb6082829df256ffc",
      "6e8836ff479f4260916e80f8e0c51095",
      "bb7df43eb63a414c8dce411ec27f5988",
      "00aef682fd7b46e2aab39785782ce2e2",
      "282d743f93584145b69efca71d0c1a6c",
      "953d8e08591042e3b59a15281c85a1cd",
      "4f6c06482aea4875a75cba7461469e82",
      "2d3a29ca48fd4c9ea0999d2398b33766",
      "b90dcc4828ea44cfbf1daa72f0384d8a",
      "038f52a2aebc4a919d4a4e2bfd414ad4"
     ]
    },
    "outputId": "58bcccb0-9e04-44df-fd21-e19ffa805f96"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"tonioexe/mistral-7b-humorous-lora\")\n",
    "model.eval()\n",
    "\n",
    "def chat(question):\n",
    "    prompt = f\"<s>[INST] {question.strip()} [/INST]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(chat(\"Give me quick tips to survive Monday mornings?\"))\n",
    "print(chat(\"How do I politely cancel plans I never wanted in the first place?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "10102c31-e491-4318-c615-b7d0ad1e4b9e"
   },
   "outputs": [],
   "source": [
    "print(chat(\"How do I politely cancel plans I never wanted in the first place?\"))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
